{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP/jwLfKmkkrlxccOPI2KET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rogerb831/Text2LIVE/blob/main/Text2LIVE_Toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optional, mount Dropbox\n",
        "!wget https://downloads.rclone.org/v1.53.2/rclone-v1.53.2-linux-amd64.deb\n",
        "!apt install ./rclone-v1.53.2-linux-amd64.deb\n",
        "!rclone config\n",
        "!sudo mkdir /content/dropbox\n",
        "!nohup rclone --vfs-cache-mode writes mount dropbox:/stable_diffusion /content/dropbox &"
      ],
      "metadata": {
        "id": "vFWjoTmQRBPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optional, mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "e-yClujbp_nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Variables & Install Dependencies"
      ],
      "metadata": {
        "id": "z_jO9RWr8mV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "#user inputs\n",
        "toolkit_path=\"/content/AI_VFX_Toolkit/data\" #@param {type:\"string\"}\n",
        "input_video_path= \"/content/gdrive/MyDrive/AI_VFX_Toolkit/il-short.mp4\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Assume some paths to simplify user experience\n",
        "frames_dir = toolkit_path +'/frames/'\n",
        "flow_dir = toolkit_path +'/frames_flow/'\n",
        "maskrcnn_dir = toolkit_path +'/frames_maskrcnn/'\n",
        "\n",
        "#Make sure dirs exist\n",
        "alldirs = [frames_dir, flow_dir, maskrcnn_dir]\n",
        "for dir in alldirs:\n",
        "  if not os.path.exists(dir):\n",
        "   os.makedirs(dir, mode=0o777, exist_ok=False)\n",
        "\n",
        "\n",
        "#Install deps\n",
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!git clone https://github.com/rogerb831/layered-neural-atlases.git\n",
        "!git clone https://github.com/rogerb831/Text2LIVE.git -b Colab-Enhancements\n",
        "%cd /content/layered-neural-atlases\n",
        "!pip install -r requirements.txt\n",
        "!git submodule update --init\n",
        "!cd thirdparty/RAFT/; sh ./download_models.sh\n",
        "%cd /content/Text2LIVE\n",
        "!sed -i '/torch/d' requirements.txt #Hack for colab compatibility\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "YFh3CWkU-C6i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract frames from video\n",
        "\n",
        "video = cv2.VideoCapture(input_video_path)\n",
        "success = True\n",
        "count = 1\n",
        "while success:\n",
        "  success,frame = video.read()\n",
        "  #\n",
        "  name = frames_dir+str(count).zfill(5)+'.jpg'\n",
        "  if success == True:\n",
        "    cv2.imwrite(name,frame)\n",
        "    print('Frame {} Extracted Successfully'.format(count))#, end='\\r')\n",
        "    count = count+1\n",
        "  else:\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "qSnbJEXttaB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Masks\n",
        "\n",
        "#@markdown You need to define a class which will be used to identify the foreground object from the background. [Click here](https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/) for a full list of classes\n",
        "class_name= \"person\" #@param {type:\"string\"}\n",
        "%cd /content/layered-neural-atlases\n",
        "print(\"frames_dir = \" +frames_dir)\n",
        "print(\"class_name = \" +class_name)\n",
        "!python preprocess_mask_rcnn.py --vid-path {frames_dir} --class_name {class_name}"
      ],
      "metadata": {
        "id": "lbkcStJA5gWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Optical Flows \n",
        "#@markdown This step Requires around 40GB of GPU memeory (premium colab GPU). It creates an npy file for each frame transition, which are around 16Mb per frame. Be sure you have the available disk space and GPU memory or it will fail.\n",
        "%cd /content/layered-neural-atlases\n",
        "max_long_edge = round(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "\n",
        "#max_long_edge = 768 #@param {type:\"string\"}\n",
        "!python preprocess_optical_flow.py --vid-path {frames_dir} --max_long_edge {max_long_edge}"
      ],
      "metadata": {
        "id": "OP4TNorDEEFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Custom Video\n",
        "Training consists of two step.\n",
        "\n",
        "\n",
        "1.   Create a pretained model using layered-neural-atlases\n",
        "2.   Train the final output using the pretrained model and Test2LIVE\n",
        "\n"
      ],
      "metadata": {
        "id": "HqvRorGi37Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pretrail Custom model\n",
        "%cd /content/layered-neural-atlases\n",
        "!python train.py config/config.json"
      ],
      "metadata": {
        "id": "yIEQryQ7MYhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test2Live trainin\n",
        "#@markdown Under development. If you want to test training, use the exmaple block below.\n"
      ],
      "metadata": {
        "id": "4JfAgsD74eR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train example video\n",
        "\n",
        "Run this block if you want to train using the demo pretrained model against the demo videos."
      ],
      "metadata": {
        "id": "-KBYF03V9EGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download model & sample data\n",
        "%cd /content/Text2LIVE\n",
        "!gdown --fuzzy -O data.zip \"https://drive.google.com/uc?id=1osN4PlPkY9uk6pFqJZo8lhJUjTIpa80J&export=download\";\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "VefelLHH_1tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_video.py --example_config car-turn_winter.yaml"
      ],
      "metadata": {
        "id": "XXZELpNbUF3m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}